[{"categories":["Projects"],"content":"Network Analysis of DC Metro System","date":"2021-04-13","objectID":"/dc-metro-analysis/","tags":null,"title":"DC Metro Network Analysis","uri":"/dc-metro-analysis/"},{"categories":["Projects"],"content":"Network analysis is often applied to the social and online systems around us, for example, Facebook friend networks. A more concrete example of a physical network, however, is a metro system. The DC metro is a straightforward example of a fully connected network system. The network can be described as nodes (stations) and edges (railway paths). With each additional connection to other stations, a station‚Äôs importance to the network becomes more critical. Given this information, how can we understand the importance of each metro station? Metro nodes can be described with three main metrics: Degree: How many other stations a rider can reach from a given station Betweenness Centrality: How much control a station has on the ability for a rider to reach other stations Closeness Centrality: How close a given station is to other stations To understand the degree, betweenness, and closeness of a given station, we first need to represent the metro system in a dataset that the computer can understand. The best way to accomplish this is through an adjacency matrix of stations. Luckily, the Washington Metropolitan Area Transit Authority (WMATA) provides a series of API products that make the fetching of station and line data easy. Code for how to build the adjacency matrix is in my Github Repo. I was surprised this data was not available anywhere I looked online, so I hope it is useful for anyone else who wants to explore metro analytics. Once the network is expressed in an adjacency matrix, Python can help us calculate the network statistics discussed above. Finally, we can overlay those stats on top of network line and station shapefiles provided by WMATA and visualized via geopandas. Hover over points to view node statistics and press the magnifying glass to zoom. Before I started this project, I assumed Metro Center would be the most important station based on my ridership patterns while living in DC. However, given the findings on this analysis, it looks like L‚ÄôEnfant Plaza is the most important! L‚ÄôEnfant has a degree of 5, betweenness of .571, and closeness of .144, not to mention an underground food court. Honorable mention goes to the Pentagon, the most important station outside of DC in terms of betweenness and closeness centrality. Instead of using set shapefiles to draw the network, what if we let the computer decide how it should look? Force-directed algorithms allow us to visualize networks as nodes and edges that repel against each other based on a given charge. The D3 JavaScript library is a popular tool for building force-directed networks ‚Äî I implemented D3 to create a free-flowing DC metro system below. Pull a node with your mouse and watch the network update positions. .links line { stroke: #999; stroke-opacity: 0.6; } .nodes circle { stroke: #fff; stroke-width: 1.5px; } text { font-family: sans-serif; font-size: 8px; } var svg = d3.select(\"svg\"), width = +svg.attr(\"width\"), height = +svg.attr(\"height\"); var color = d3.scaleOrdinal(d3.schemeCategory20); var simulation = d3.forceSimulation() .force(\"link\", d3.forceLink().id(function(d) { return d.id; })) .force(\"charge\", d3.forceManyBody().strength(-15)) .force(\"center\", d3.forceCenter(width / 2, height / 2)); d3.json(\"wmata.json\", function(error, graph) { if (error) throw error; var link = svg.append(\"g\") .attr(\"class\", \"links\") .selectAll(\"line\") .data(graph.links) .enter().append(\"line\") .attr(\"stroke-width\", function(d) { return Math.sqrt(d.degree); }); var node = svg.append(\"g\") .attr(\"class\", \"nodes\") .selectAll(\"g\") .data(graph.nodes) .enter().append(\"g\") var circles = node.append(\"circle\") .attr(\"r\", function(d) { return d.degree * 3; }) .attr(\"fill\", function(d) { return color(d.degree); }) .call(d3.drag() .on(\"start\", dragstarted) .on(\"drag\", dragged) .on(\"end\", dragended)); var lables = node.append(\"text\") .text(function(d) { return d.id; }) .attr('x', 6) .attr('y', 3); node.append(\"title\") .text(function(d) { return d.id; }); simulation .nodes(graph.nodes) .o","date":"2021-04-13","objectID":"/dc-metro-analysis/:0:0","tags":null,"title":"DC Metro Network Analysis","uri":"/dc-metro-analysis/"},{"categories":["Posts"],"content":"Updated website","date":"2022-01-09","objectID":"/switch-to-blogdown/","tags":null,"title":"Switch to Blogdown","uri":"/switch-to-blogdown/"},{"categories":["Posts"],"content":"In 2022, my goal is to post more content here, so it felt appropriate to give my website a facelift. This new site is built in blogdown, a package used to create websites in RMarkdown. My previous website was built with Jekyll around two years ago, and I didn‚Äôt put much time into designing the site or understanding how to best use Jekyll. Re-starting with the Blogdown framework gave me an opportunity to start from scratch and build something I am proud of. There are a few things about blogdown I really like: It‚Äôs built for R users and super intuitive if you are coming from an R background Hugo themes. It‚Äôs super easy to create a good looking site without getting caught up in details. Works well with github pages, where my site is currently hosted Site Redesign TLDR; blogdown is awesome, I highly recommend to anyone wanting to build a new website! ","date":"2022-01-09","objectID":"/switch-to-blogdown/:0:0","tags":null,"title":"Switch to Blogdown","uri":"/switch-to-blogdown/"},{"categories":["Posts"],"content":"Playing around with OpenCV and disc golf","date":"2022-01-02","objectID":"/disc-video-recognition/","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Posts"],"content":"Motivation I was first introduced to pro disc golf via JomezPro, a production company that posts disc golf tournament coverage on YouTube. Since there is usually not free access to live pro tournament coverage, companies like JomezPro film pro disc golf rounds, edit the footage with graphics and commentary same day, then post the coverage on Youtube the following morning for fans to watch the previous days action. In 2020, watching JomezPro quickly became part of my weekend morning ritual somewhere between my first cup of coffee, homework, and getting out to play my local course in Raleigh. I would say three things make JomezPro special: Filming rights to lead cards (the top 4 players by score in a given tournament) Great commentary by pros, usually Paul Ulibarri, Jeremy Koling, and Nate Sexton Top-notch production, highlighted by Follow Flight segments ","date":"2022-01-02","objectID":"/disc-video-recognition/:0:1","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Posts"],"content":"Follow Flight Follow Flight is used when a player throws a great shot, generally a drive or long approach, and parks the disc under the basket. It is basically the slow motion dunk NBA replay equivalent to disc golf. An editor at JomezPro is tasked with editing the shot footage to follow the disc flight with a florescent ribbon effect so viewers can visualize the full flight of the disc and look with awe at the shot difficulty. I‚Äôm honestly not sure how long something like this takes from a video editing POV, but these comments make it seem like a decent amount of work. So, my question and purpose for this post is, can Follow Flight be automated? Hypothesis: Yes. Conclusion: Maybe, but probably not, at least by me right now. ","date":"2022-01-02","objectID":"/disc-video-recognition/:0:2","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Posts"],"content":"Computer vision and discs Computer vision (CV) models have gotten pretty good at object detection and recognition in both image and video content.To automate a Follow Flight effect, you need to track a disc and animate a line through the entire flight. CV models are a bit outside my expertise but hey, I have access to Google and too much free time. CV models are generally built with Neural Networks and can be computationally expensive to train from scratch. My couch-based research led me to ImageAI, a project consisting of open source tools to support image prediction, custom image prediction, object detection, video detection, video object tracking and image predictions trainings. From what I‚Äôve read, ImageAI is a great place to start tinkering with CV as they provide a simple Python API and interface easily with pre-trained image recognition models like yolovX and RetinaNet. Back to the problem at hand - can we use CV to track a disc through a video? I happen to have a video of myself throwing at Maple Hill DG course that will work for this (ignore the part where I throw into the water).Using ImageAI, we can run a custom frisbee detection model from RetinaNet on the input video and recieve a tagged output. This took about 15 minutes to run on my Macbook and was only ~20 lines of Python code, pretty cool how easy it is to get up and running! ","date":"2022-01-02","objectID":"/disc-video-recognition/:0:3","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Posts"],"content":"Results The model can only briefly identify the disc in my hand at the begining of the video, then completely loses the disc once it leaves my hand.This is certainly not as good as I hoped. There are a few problems right off the bat here. Image quality is not great as this was shot on an Iphone, not a legit camera I tuned literally no hyper-parameters, this is as out of the box as you get Training data as noted in code line 13 was based on ‚Äúfrisbees‚Äù. I‚Äôm wondering if this pre-trained model was trained only on the Wham-O/Ultimate/play catch with your dog variety of frisbee, and if there were any disc golf discs such as mine (lost forever now) included in the training data. Next steps here seem pretty clear - if out of the box model is bad, maybe a custom trained model would do better? I have not gotten that far, but downloading a bunch of professionally shot disc golf video content and self tagging the training data for a few hours sounds awesome and terrible at the same time. It would be cool to do something like this at a larger scale with a service like GCP Video API. ","date":"2022-01-02","objectID":"/disc-video-recognition/:0:4","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Posts"],"content":"Parting thoughts This was a cool intro to computer vision and I hope to dive a bit deeper into custom model development and applications to daily life. Based on what I‚Äôve read I think the biggest value in CV is for instant classification applications like self-driving car systems, but automation of disc golf video editing is a close second. Code is below and was adapted from the ImageAI docs. from imageai.Detection import VideoObjectDetection #image ai package import os import time # Initialize detector execution_path = os.getcwd() detector = VideoObjectDetection() # Load RetinaNet model detector.setModelTypeAsRetinaNet() detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\")) detector.loadModel() custom_objects = detector.CustomObjects(frisbee=True) # detect only frisbees # Run RetinaNet model ret_time_start = process_time() # timer video_path = detector.detectCustomObjectsFromVideo( custom_objects=custom_objects, input_file_path=os.path.join(execution_path, \"inputs/maple_hill_input.mp4\"), output_file_path=os.path.join(execution_path, \"outputs/mapple_hill_ret\"), frames_per_second=20, log_progress=True) ret_time_end = process_time() ret_run_time = ret_time_end - ret_time_start print(ret_run_time) ","date":"2022-01-02","objectID":"/disc-video-recognition/:0:5","tags":null,"title":"Pro Disc Golf, Follow Flight, and Computer Vision","uri":"/disc-video-recognition/"},{"categories":["Projects"],"content":"Python implementation of DC Metro API on LED Board","date":"2021-06-17","objectID":"/dc-metro-prediction-board/","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"Overview In April, I wrote about a network analysis of the DC metro system. In the process of finding metro data for that project, I gained developer access to the WMATA metro API. One endpoint of the API was current predictions for each train and station within the DC metro network, which sparked an idea for a DIY metro board for my U-Street apartment. The goal was to build something that resembles the actual train prediction screens found in DC metro stations but have it in my own living room, all made possible by the WMATA API and some hardware found on the internet. ","date":"2021-06-17","objectID":"/dc-metro-prediction-board/:1:0","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"Process Now that the project is complete, I wanted to share my process and what I learned in my first project dealing with both hardware \u0026 software. All the code is on my Github and can be cloned if you want to build a board for your own WMATA station. ","date":"2021-06-17","objectID":"/dc-metro-prediction-board/:2:0","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"Hardware: To build a real time metro prediction board, you need a screen, a computer to connect to the screen and API, and a power connection. Hardware costs $118 if you buy everything new from Adafruit Raspberry Pi ZeroWH Raspberry Pi with WIFI access and GPIO headers to connect to the AdaFruit Bonnet. Any Raspberry Pi / Arduino with these specs would work, but this was the cheapest option I could find 16GB Micro SD Card SD card used for the Raspberry Pi OS and storage 64 x 32 RGB LED Matrix - 6mm pitch LED Board that displays metro predictions. The pitch is the space between pixels, I choose 6mm Adafruit RGB Matrix Bonnet for Raspberry Pi Bonnet hardware between Raspberry Pi and LED board Basic architecture for the system is displayed below: ","date":"2021-06-17","objectID":"/dc-metro-prediction-board/:2:1","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"Software: I thought about software in two groups: code to pull and store train predictions from the WMATA API, and code to display train predictions on the LED board. Requirements: Pull and store train predictions from WMATA API Connect to the WMATA API through WIFI Update every 10 seconds Don‚Äôt get rate limited Store predictions as PNG Allow user to change station as needed Requirements: Display train predictions on LED board Make sure text fits on board Refresh text every 10 seconds Pulling WMATA train predictions for a given station every 10 seconds was simple enough through the WMATA‚Äôs developer API. The harder part was figuring out how to display and update the predictions on the LED board. Luckily, the rpi-rgb-led-matrix library built by Henner Zeller can be used to control LED panels from a computer like a Raspberry Pi. With the rpi-rgb-led-matrix Python bindings, you can choose to display scrolling text or static images on a single board or string of boards. I preferred the PNG image option so I could display three incoming train predictions at once, which meant I needed to convert JSON API responses to a PNG image file the rpi-rgb-led-matrix library could handle. I‚Äôve played with scraping text off images with OCR packages but had never thought of converting a text to an image. Luckily, the Python Imaging Library (PIL/Pillow) simplified this process to a few lines of code. A basic process flow for the system is outlined below. The Prod_Prediction_Ticker.py and Prod_Display_Image.py files run concurrently and refresh every 10 seconds. ","date":"2021-06-17","objectID":"/dc-metro-prediction-board/:2:2","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"What I learned Building something physical is super rewarding Most of my projects end up as a model or dashboard. It was fun to make something with a physical end product that I can keep on my desk Cloud services are awesome Imaging and setting up a Raspberry Pi for an SSH connection take time and know how. This really puts into perspective how nice it is to spin up an EC2 instance and be ready to code in a couple minutes. Working with a new Linux computer first hand gave me an appreciation for what goes on behind the scenes with VMs. Linux is nice to know Everything I did on the Raspberry Pi from downloading libraries, scping files, updating python scripts was through the command line. A basic understanding of the useful commands was super important to work quickly. I included useful commands in this project‚Äôs Github repo that I used throughout this project. If you are interested in building one of these for DC or another city with real-time accessible data, let me know! ","date":"2021-06-17","objectID":"/dc-metro-prediction-board/:3:0","tags":null,"title":"DC Metro Prediction Board","uri":"/dc-metro-prediction-board/"},{"categories":["Projects"],"content":"Which scratch off should you buy? It's not **all** luck","date":"2020-12-01","objectID":"/scratcherstats/","tags":null,"title":"ScratcherStatsNC","uri":"/scratcherstats/"},{"categories":["Projects"],"content":"ScratcherStatsNC Link ","date":"2020-12-01","objectID":"/scratcherstats/:1:0","tags":null,"title":"ScratcherStatsNC","uri":"/scratcherstats/"},{"categories":["Projects"],"content":"Motivation Not all scratch off tickets are created equal. I started this project when I was gifted a bunch of scratchers for my birthday and got all losers, which felt unlikely. The basic idea here is that performance is not equal amongst tickets, and that performance changes through time as people win and lose prizes. By collecting daily lottery data and making a few estimations, I can find which tickets are likely to offer the most bang for your buck. Hope you enjoy, and good luck! ","date":"2020-12-01","objectID":"/scratcherstats/:1:1","tags":null,"title":"ScratcherStatsNC","uri":"/scratcherstats/"},{"categories":["Projects"],"content":"Data, Viz, \u0026 Automation Data is scraped hourly from NC Lottery Website via an EC2 instance I use for my side projects. Data is then sent to a Shiny App hosted on shinyapps.io. The Shiny app is a mix of HTML, R, and Plotly code. The project was launched in Winter 2020 and continues to update daily. ","date":"2020-12-01","objectID":"/scratcherstats/:1:2","tags":null,"title":"ScratcherStatsNC","uri":"/scratcherstats/"},{"categories":null,"content":"I‚Äôm a data scientist and alum of the Institute for Advanced Analytics. I enjoy building full-stack data products that uncover things new, interesting, and important! I love to chat about the below topics, feel free to reach out üòÅ Rstats Economics Dunkin Donuts Italian food Disc golf (where i‚Äôve played) Biking Italy, China, DC, Raleigh ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"About ","uri":"/about/"}]